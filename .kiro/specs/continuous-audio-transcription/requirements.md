# Requirements Document

## Project Description (Input)

24時間連続音声文字起こしシステム仕様

### システム概要
BlackHole経由で取得したZoom/Discord等の音声をWhisper.cppでリアルタイム文字起こしする常時稼働システムを構築する。LadioCastで音声ルーティングを行い、自分の声も相手の声も両方記録できる構成とする。

### 基本設計思想

**メモリ効率最優先**
- 24時間稼働を前提とし、メモリリークを徹底的に防ぐ
- 30秒ごとにバッファをフラッシュして処理
- Whisper.cppのstreamモードではなくmainモードを使用（安定性重視）

**エラー耐性**
- 音声入力が途切れても自動復帰
- Whisper.cppがクラッシュしても再起動
- 1時間に1回プロセス再起動で予防的メンテナンス

### 音声処理フロー
1. FFmpegでBlackHole 2chから音声取得（16kHz、モノラル変換）
2. 30秒バッファに蓄積
3. 無音判定（10秒以上無音なら処理スキップ）
4. Whisper.cpp実行（mediumモデル、日本語指定）
5. 結果をJSONL形式で保存
6. 1時間ごとにテキストファイルにもエクスポート

### ファイル管理戦略

**ログファイル構成**
```
~/transcriptions/
├── 2025-01-27/
│   ├── raw/           # 生の文字起こし（30秒ごと）
│   ├── hourly/        # 1時間ごとの結合版
│   ├── daily.txt      # 日次サマリー
│   └── audio/         # 重要部分の音声保存（オプション）
```

**データ保持ポリシー**
- 生データ: 7日間
- 時間単位データ: 30日間
- 日次サマリー: 永続保存
- 3日経過したらgzip圧縮

### 無音検出とコスト最適化

**VAD（Voice Activity Detection）実装**
- 簡易版: 音声レベルの閾値判定
- 高度版: webrtcvadライブラリ使用
- 10秒以上無音が続いたらWhisper.cpp実行をスキップ
- ただし5分に1回は強制実行（接続確認）

### 文字起こし品質向上

**後処理パイプライン**
- 基本的な句読点挿入（ルールベース）
- 「あー」「えー」などのフィラー除去（オプション）
- 話者交代の検出（音声特徴量の変化から推定）
- 1時間ごとにClaude APIで整形（コスト許容なら）

### システム監視

**ヘルスチェック項目**
- メモリ使用量（2GB超えたらアラート）
- CPU使用率（50%超えたら警告）
- 最終文字起こし時刻（5分以上更新なければ異常）
- エラー発生率（1時間に10回以上でアラート）

**通知設定**
- 異常時: Slack/Discord webhook
- 日次レポート: 文字起こし文字数、稼働時間
- 週次: ストレージ使用量、エラー統計

### 実装の優先順位

**フェーズ1（MVP）**
- BlackHoleから音声取得
- Whisper.cppで30秒ごと文字起こし
- テキストファイル保存
- 基本的なエラーハンドリング

**フェーズ2（安定化）**
- 無音検出
- メモリ監視
- 自動再起動
- ログローテーション

**フェーズ3（高度化）**
- 話者分離
- リアルタイムWeb UI
- n8n連携で自動要約
- 重要キーワード検出時のアラート

### 想定される課題と対策

**長時間稼働での課題**
- メモリリーク → 定期的なプロセス再起動
- ディスク容量 → 自動圧縮とローテーション
- Whisper.cppのハング → タイムアウト設定

**精度の課題**
- 専門用語 → カスタム辞書機能
- 複数話者 → 将来的にpyannotate統合
- ノイズ → RNNoiseでの前処理

### 開発時の注意事項
- まず1時間の連続稼働を確認してから24時間に拡張
- メモリプロファイラで定期的にリーク確認
- エラー時は音声バッファを一時保存して後で再処理可能に
- Whisper.cppのモデルサイズは環境に応じて調整（small/medium/large）

## はじめに

本システムは、BlackHole仮想オーディオデバイス経由でZoom/Discord等の音声会議をキャプチャし、WhisperXを用いて24時間連続で文字起こしを行うTypeScript製CLIアプリケーションです。**話者分離機能を標準搭載**し、複数話者の発話を自動的に区別します。将来的には音声プロファイルによる話者識別にも対応予定です。

**ビジネス価値**: 会議・通話内容を話者付きで自動記録することで、議事録作成の負担を大幅に削減し、誰が何を発言したかを明確に把握できます。

**技術スタック**:
- クライアント（Mac）: TypeScript/Node.js、FFmpeg
- サーバー（CUDAマシン）: Python、WhisperX、pyannote.audio

**システム構成**:
- Mac: 音声キャプチャ、バッファリング、ファイル管理
- CUDAマシン（LAN経由）: 文字起こし + 話者分離の高速処理

## 用語定義

### 話者分離（Speaker Diarization）
音声を「誰が話しているか」でセグメント分けする処理。同一音声ファイル内で話者を区別し、Speaker_00、Speaker_01等の匿名ラベルを付与する。セッション間での話者の追跡は行わない。

**出力例:**
```
[Speaker_00] こんにちは
[Speaker_01] よろしくお願いします
[Speaker_00] では始めましょう
```

**特徴:**
- WhisperX/pyannote.audioの標準機能
- 実装難易度: 低
- 処理速度: 高速（2-3秒/30秒音声）
- セッション独立: 次の音声では別のラベルになる可能性あり

### 話者識別（Speaker Identification）
事前に登録された音声プロファイルと照合し、話者に具体的な名前を付与する処理。「Speaker_00 = 山本さん」のように、セッション間で同一人物を追跡可能にする。

**出力例:**
```
[山本さん] こんにちは
[田中さん] よろしくお願いします
[山本さん] では始めましょう
```

**特徴:**
- 話者埋め込みベクトルによる照合が必要
- 実装難易度: 中〜高
- 処理速度: やや遅い（追加処理が必要）
- セッション継続: 同一人物を一貫して識別可能

**本仕様での位置づけ:**
- フェーズ1: 話者分離のみ実装（WhisperX標準機能）
- フェーズ2以降: 話者識別を追加（オプション機能、要件10参照）

## 要件

### 要件1: 音声キャプチャとバッファリング
**目的:** システムオペレーターとして、BlackHoleデバイスから音声を取得し、安定的にバッファリングしたい。これにより、音声の取りこぼしなく文字起こしの準備ができる。

#### 受入基準

1. WHEN 文字起こしサービスが起動された THEN システムはBlackHole 2chオーディオデバイスからFFmpegを使用して音声入力を開始しなければならない
2. WHEN 音声入力が開始された THEN システムは音声を16kHz、モノラル形式に変換しなければならない
3. WHILE 音声が入力されている THE システムは30秒間の音声データをメモリバッファに蓄積し続けなければならない
4. WHEN 30秒のバッファが満たされた THEN システムはバッファ内容を一時ファイルとして保存しなければならない
5. WHEN バッファが保存された THEN システムはメモリバッファを解放（null化）し、次の30秒バッファの蓄積を開始しなければならない
6. IF BlackHoleデバイスが利用できない THEN システムはエラーログを出力し、10秒後に再接続を試行しなければならない
7. WHEN 音声入力が3回連続で失敗した THEN システムは管理者に通知を送信しなければならない

### 要件2: 無音検出と処理最適化
**目的:** システムオペレーターとして、無音区間でのWhisper.cpp実行をスキップしたい。これにより、CPU使用率とコストを削減できる。

#### 受入基準

1. WHEN 30秒バッファが保存された THEN システムは音声レベルを分析して無音判定を実行しなければならない
2. IF 音声バッファ内の音声レベルが閾値（-40dB）を下回る状態が10秒以上続いた THEN システムは当該バッファの文字起こし処理をスキップしなければならない
3. WHEN 文字起こし処理がスキップされた THEN システムはスキップ理由と時刻をログに記録しなければならない
4. WHILE 無音状態が継続している THE システムは5分ごとに強制的に文字起こしを1回実行して音声入力の接続を確認しなければならない
5. WHEN 音声が検出された THEN システムは通常の文字起こし処理を再開しなければならない

### 要件3: 文字起こし処理（リモートCUDA実行）
**目的:** システムオペレーターとして、LAN内のCUDAサーバーで高速・高精度に文字起こしを実行したい。これにより、Macのリソースを節約しながら高品質な文字起こしを得られる。

#### 受入基準

1. WHEN 30秒の音声バッファが文字起こし対象となった THEN システムはCUDAサーバーに音声ファイルをHTTP POSTで送信しなければならない
2. WHEN CUDAサーバーにリクエストを送信する THEN システムはタイムアウトを60秒に設定しなければならない
3. IF CUDAサーバーが60秒以内に応答しない THEN システムはタイムアウトエラーをログに記録し、次のバッファ処理を継続しなければならない
4. IF CUDAサーバーへの接続が3回連続で失敗した THEN システムは管理者に通知を送信しなければならない
5. WHEN 文字起こし結果を受信した THEN システムは話者ラベル付きセグメント情報をJSONL形式で`~/transcriptions/{YYYY-MM-DD}/raw/`ディレクトリに保存しなければならない
6. WHEN JSONL形式で保存する THEN システムは各レコードにタイムスタンプ、音声ファイルパス、文字起こしテキスト、話者ID、セグメント開始/終了時刻を含めなければならない
7. WHERE 設定ファイルにローカルフォールバックが有効化されている THE システムはCUDAサーバー障害時にローカルWhisper.cppで処理を継続しなければならない

### 要件4: ファイル管理とローテーション
**目的:** システムオペレーターとして、文字起こし結果を適切に整理・保存したい。これにより、ディスク容量を効率的に管理しながら必要な記録を保持できる。

#### 受入基準

1. WHEN システムが起動した THEN システムは`~/transcriptions/{YYYY-MM-DD}/`形式で当日のディレクトリ構造（raw/、hourly/）を作成しなければならない
2. WHEN 1時間が経過した THEN システムはその時間帯の全JSONL記録を結合し、`hourly/{HH}.txt`形式でプレーンテキストファイルを生成しなければならない
3. WHEN hourlyファイルを生成する THEN システムは時系列順にソートされたテキストを出力しなければならない
4. WHEN 日付が変わった THEN システムは前日の全hourlyファイルを結合して`daily.txt`を生成しなければならない
5. WHEN 生データが7日以上経過した THEN システムはraw/ディレクトリ内の該当ファイルを自動削除しなければならない
6. WHEN 時間単位データが30日以上経過した THEN システムはhourly/ディレクトリ内の該当ファイルを自動削除しなければならない
7. WHEN データが3日以上経過した THEN システムは該当ファイルをgzip圧縮しなければならない
8. IF ディスク空き容量が10GB未満になった THEN システムは警告ログを出力し、管理者に通知を送信しなければならない

### 要件5: プロセス管理と自動復旧
**目的:** システムオペレーターとして、24時間安定稼働を実現したい。これにより、手動介入なしで長期間の運用が可能になる。

#### 受入基準

1. WHILE システムが稼働している THE システムは1時間ごとに予防的なプロセス再起動を実行しなければならない
2. WHEN プロセス再起動を実行する THEN システムは現在のバッファ内容を保存してから再起動しなければならない
3. WHEN プロセス再起動が完了した THEN システムは再起動完了ログを記録し、音声キャプチャを自動再開しなければならない
4. IF システムプロセスが予期せず終了した THEN プロセスマネージャー（pm2等）がシステムを自動的に再起動しなければならない
5. WHEN システムが再起動した THEN システムは前回の処理状態を復元し、未処理バッファがあれば処理を継続しなければならない
6. WHILE システムが稼働している THE システムはNode.jsのヒープメモリ上限を2GBに制限しなければならない

### 要件6: システム監視とヘルスチェック
**目的:** システムオペレーターとして、システムの健全性を常に把握したい。これにより、問題の早期発見と対処が可能になる。

#### 受入基準

1. WHILE システムが稼働している THE システムは1分ごとにメモリ使用量をチェックしなければならない
2. IF メモリ使用量が2GBを超えた THEN システムは警告ログを出力し、Webhook経由で管理者に通知を送信しなければならない
3. WHILE システムが稼働している THE システムは1分ごとにCPU使用率をチェックしなければならない
4. IF CPU使用率が50%を5分間連続で超えた THEN システムは警告ログを出力しなければならない
5. WHILE システムが稼働している THE システムは最終文字起こし時刻を追跡しなければならない
6. IF 最終文字起こしから5分以上経過した AND 音声入力が継続している THEN システムは異常ログを出力し、管理者に通知を送信しなければならない
7. WHILE システムが稼働している THE システムは1時間ごとにエラー発生回数をカウントしなければならない
8. IF エラー発生回数が1時間に10回を超えた THEN システムは緊急アラートを管理者に送信しなければならない

### 要件7: 通知と日次レポート
**目的:** システムオペレーターとして、システムの稼働状況を定期的に把握したい。これにより、長期的な運用状況を可視化できる。

#### 受入基準

1. WHEN システムが異常を検出した THEN システムはSlack/Discord WebhookにJSON形式で異常内容を送信しなければならない
2. WHEN 1日が終了した THEN システムは日次レポート（文字起こし文字数、稼働時間、エラー回数）を生成しなければならない
3. WHEN 日次レポートが生成された THEN システムはWebhook経由で管理者にレポートを送信しなければならない
4. WHEN 7日が経過した THEN システムは週次レポート（ストレージ使用量、平均CPU/メモリ使用率、エラー統計）を生成しなければならない
5. WHEN 週次レポートが生成された THEN システムはWebhook経由で管理者にレポートを送信しなければならない

### 要件8: 話者分離（優先度：高）
**目的:** システムオペレーターとして、複数の話者を自動的に区別したい。これにより、会話の流れと話者交代を明確に記録できる。

#### 受入基準

1. WHERE 設定ファイルで話者分離が有効化されている THE システムは文字起こし処理時にLAN内のCUDAサーバーに音声ファイルを送信しなければならない
2. WHEN CUDAサーバーにリクエストを送信する THEN システムは30秒以内にレスポンスを受信しなければならない
3. IF CUDAサーバーが応答しない OR タイムアウトした THEN システムは話者ラベルなしで文字起こし結果を保存しなければならない
4. WHEN 話者分離結果を受信した THEN システムは各セグメントに匿名話者ラベル（Speaker_00、Speaker_01等）を付与しなければならない
5. WHEN JSONL形式で保存する THEN システムは各レコードに話者情報（speaker_id）を含めなければならない
6. WHEN hourlyファイルを生成する THEN システムは`[Speaker_00] テキスト`形式で話者ラベル付きテキストを出力しなければならない
7. WHERE 設定ファイルに`diarization.server_url`が設定されている THE システムは起動時にCUDAサーバーのヘルスチェックを実行しなければならない
8. IF ヘルスチェックが失敗した THEN システムは警告ログを出力し、話者分離なしモードで起動を継続しなければならない

### 要件9: CLI操作と設定管理
**目的:** システムオペレーターとして、CLIで簡単にシステムを操作したい。これにより、起動・停止・設定変更を効率的に実行できる。

#### 受入基準

1. WHEN オペレーターが`transcribe start`コマンドを実行した THEN システムはバックグラウンドでデーモンプロセスを起動しなければならない
2. WHEN オペレーターが`transcribe stop`コマンドを実行した THEN システムは現在のバッファを保存してから安全にプロセスを停止しなければならない
3. WHEN オペレーターが`transcribe status`コマンドを実行した THEN システムは現在の稼働状態（起動時刻、メモリ使用量、最終文字起こし時刻、話者分離の有効/無効）を表示しなければならない
4. WHEN オペレーターが`transcribe logs`コマンドを実行した THEN システムは直近100行のログをリアルタイム表示しなければならない
5. WHERE システムルートディレクトリに設定ファイル（config.json）が存在する THE システムは起動時に設定をロードしなければならない
6. IF 設定ファイルが存在しない THEN システムはデフォルト設定でconfig.jsonを生成しなければならない
7. WHEN 設定ファイルをロードする THEN システムは音声デバイス名、Whisperモデルパス、出力ディレクトリ、Webhook URL、話者分離サーバーURLを読み込まなければならない

### 要件10: 話者識別（優先度：低、将来機能）
**目的:** システムオペレーターとして、匿名話者ラベル（Speaker_00等）に具体的な名前を付与したい。これにより、セッション間で同一人物を追跡し、「山本さん」「田中さん」等の実名で記録できる。

**実装タイミング:** フェーズ2以降（要件1-9の実装完了後）

#### 受入基準

1. WHERE 設定ファイルで話者識別が有効化されている AND 話者プロファイルが登録されている THE システムは話者分離後に追加の識別処理を実行しなければならない
2. WHEN オペレーターが`transcribe register-voice <name>`コマンドを実行した THEN システムは音声サンプルを記録し、話者プロファイルを作成しなければならない
3. WHEN 話者プロファイルが作成された THEN システムは話者埋め込みベクトルをCUDAサーバーで生成し、ローカルデータベースに保存しなければならない
4. WHEN 話者分離結果（Speaker_00、Speaker_01等）を受信した THEN システムは各セグメントの音声特徴量を抽出しなければならない
5. WHEN 音声特徴量が抽出された THEN システムは登録済み話者プロファイルとのコサイン類似度を計算しなければならない
6. IF 類似度が閾値（0.7以上）を超えた THEN システムは匿名ラベルを実名ラベル（例: Speaker_00 → 山本さん）に置き換えなければならない
7. IF どのプロファイルとも一致しない THEN システムは匿名ラベルのまま保存しなければならない
8. WHEN hourlyファイルを生成する THEN システムは`[山本さん] テキスト`形式で実名ラベル付きテキストを出力しなければならない
9. WHEN オペレーターが`transcribe list-voices`コマンドを実行した THEN システムは登録済み話者プロファイルの一覧を表示しなければならない
10. WHEN オペレーターが`transcribe remove-voice <name>`コマンドを実行した THEN システムは指定された話者プロファイルを削除しなければならない

**技術メモ:**
- 話者識別はリアルタイムでなく、後処理として実装可能（バッチ処理で過去の文字起こしに適用）
- pyannote.audioの話者埋め込みベクトル抽出機能を使用
- 初期実装ではシンプルなJSONファイルベースのプロファイル管理で十分

## Requirements
